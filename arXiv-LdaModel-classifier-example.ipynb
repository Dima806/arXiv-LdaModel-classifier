{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv.org LdaModel classifier\n",
    "\n",
    "The script downloads abstracts for [arXiv.org](https://arxiv.org) papers, selects them by topics and, based on the topic selection, returns abstract of the paper closed to a pre-selected one.\n",
    "\n",
    "Note that **running this script is quite long**, ~1hour in total! If you want to run it faster, consider reducing number of passes in LdaModel from 50 to, e.g., 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import urllib, urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import io\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading abstracts using [arXiv.org](https://arxiv.org) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Downloading 100 chunk from 100:\n",
      "Running time is 458.092577457428s\n",
      "25399\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "myfile = open(\"log.txt\", \"w\")\n",
    "myfile.close()\n",
    "\n",
    "n_attempts = 100\n",
    "max_articles_per_attempt = 500\n",
    "for i in range(n_attempts):\n",
    "    print(\"\\r\\t>>> Downloading {} chunk from {}:\".format(i+1, n_attempts), end='')\n",
    "    n_start = i*max_articles_per_attempt\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+TO+201710011159]&sortBy=lastUpdatedDate&sortOrder=descending&&start='+str(n_start)+'&max_results='+str(max_articles_per_attempt)\n",
    "    url = requests.get(url).content\n",
    "    soup = BeautifulSoup(url,\"html.parser\")\n",
    "    with io.open('log.txt', 'a', encoding='utf8') as logfile:\n",
    "        for tr in soup.find_all('summary')[2:]:\n",
    "            logfile.write(u\"%s\\n\" % (tr.text))\n",
    "end_time = time.time()\n",
    "print(\"\\nRunning time is {}s\".format(end_time-start_time))\n",
    "\n",
    "with io.open('log.txt', 'r', encoding='utf8') as logfile:\n",
    "    s = logfile.read().split('\\n\\n')\n",
    "    #print(s)\n",
    "logfile.close()\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total size of downloaded abstracts for ~25K papers from Computer Science (cs) arXiv category is **~27 MBytes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27572255"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to create the relevant dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use CountVectorizer to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 100 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df=100, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "# Fit and transform\n",
    "X = vect.fit_transform(s)\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained dictionary contains ~2.8K words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vect.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, it contains the word \"robot\" (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"robot\" in vect.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html) to split the papers on 10 topics.\n",
    "This step is **very slow** (~45 minutes), to reduce computing time consider decreasing number of passes to, e.g., 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time is 2157.466866493225s\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "start_time = time.time()\n",
    "ldamodel = LdaModel(corpus, num_topics=n_topics, random_state=0, passes=50, id2word=id_map) # passes = 100\n",
    "end_time = time.time()\n",
    "print(\"Running time is {}s\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 major words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"control\" + 0.012*\"game\" + 0.011*\"agents\" + 0.010*\"games\" + 0.010*\"dynamics\"'),\n",
       " (1,\n",
       "  '0.030*\"learning\" + 0.020*\"neural\" + 0.016*\"deep\" + 0.015*\"training\" + 0.014*\"networks\"'),\n",
       " (2,\n",
       "  '0.016*\"security\" + 0.013*\"mobile\" + 0.012*\"networks\" + 0.012*\"devices\" + 0.011*\"privacy\"'),\n",
       " (3,\n",
       "  '0.017*\"codes\" + 0.012*\"code\" + 0.011*\"language\" + 0.009*\"systems\" + 0.009*\"theory\"'),\n",
       " (4,\n",
       "  '0.024*\"channel\" + 0.016*\"rate\" + 0.012*\"power\" + 0.012*\"information\" + 0.010*\"scheme\"'),\n",
       " (5,\n",
       "  '0.026*\"graph\" + 0.016*\"graphs\" + 0.015*\"number\" + 0.012*\"log\" + 0.012*\"set\"'),\n",
       " (6,\n",
       "  '0.012*\"energy\" + 0.011*\"memory\" + 0.009*\"computing\" + 0.009*\"distributed\" + 0.009*\"algorithms\"'),\n",
       " (7,\n",
       "  '0.010*\"social\" + 0.010*\"information\" + 0.010*\"research\" + 0.009*\"analysis\" + 0.008*\"users\"'),\n",
       " (8,\n",
       "  '0.031*\"image\" + 0.023*\"images\" + 0.016*\"detection\" + 0.016*\"object\" + 0.013*\"video\"'),\n",
       " (9,\n",
       "  '0.010*\"matrix\" + 0.010*\"methods\" + 0.009*\"algorithms\" + 0.008*\"linear\" + 0.008*\"optimization\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=n_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding abstract of a new paper [\"Line Formation by Fat Robots under Limited Visibility\"](https://arxiv.org/abs/1710.09398):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = [\"This paper proposes a distributed algorithm for a set of tiny unit disc shaped robot \\\n",
    "to form a straight line. The robots are homogeneous, autonomous, anonymous. They observe their \\\n",
    "surrounding up to a certain distance, compute destinations to move to and move there. They do not have any \\\n",
    "explicit message sending or receiving capability. They forget their past observed or computed data. \\\n",
    "The robots do not have any global coordinate system or origin. Each robot considers its position as its origin. \\\n",
    "However, they agree on the X and Y axis. The robots are not aware of the total number of robots in the system. \\\n",
    "The algorithm presented in this paper assures collision free movements of the robots. \\\n",
    "To the best of our knowledge this paper is the first reported result on line formation by fat robots \\\n",
    "under limited visibility.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics and corresponding scores of a given abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.699768692661\n",
      "2 0.0803946568701\n",
      "5 0.204939500149\n"
     ]
    }
   ],
   "source": [
    "def topic_distribution(doc, num_topics = n_topics):\n",
    "    \n",
    "    topics_array = np.zeros(num_topics)\n",
    "    X1 = vect.transform(doc)\n",
    "    corpus1 = gensim.matutils.Sparse2Corpus(X1, documents_columns=False)\n",
    "    list_1 = list(ldamodel.get_document_topics(bow=corpus1))[0]\n",
    "    for items in list_1:\n",
    "        topics_array[items[0]] = items[1]\n",
    "    return topics_array\n",
    "for i, item in enumerate(topic_distribution(new_doc)):\n",
    "    if item > 0:\n",
    "        print(i, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the final \"closeness score\", I use the simple dot product of individual topic scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(doc1, new_doc):\n",
    "    \n",
    "    return np.dot(topic_distribution(doc1), topic_distribution(new_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return abstract of a paper, maximally resembling that of a given paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_max_resembling_paper():\n",
    "    \n",
    "    max_score = 0\n",
    "    start_time = time.time()\n",
    "    for i, doc in enumerate(s):\n",
    "        score = calculate_score(new_doc, [doc])\n",
    "        print(\"\\r\\t>>> Progress\\t:{:.4%}\".format((i + 1)/len(s)), end='')\n",
    "        #print(i, score, topic_distribution([doc]))\n",
    "        if score > max_score:\n",
    "            best_i = i\n",
    "            max_score = score\n",
    "    end_time = time.time()\n",
    "    print(\"\\nRunning time is {}s\".format(end_time-start_time))\n",
    "    return max_score, best_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Progress\t:100.0000%\n",
      "Running time is 99.09927582740784s\n",
      ">>> Best score is: 0.687648686163\n",
      "Abstract of the closest paper:\n",
      "  This paper proposes control laws ensuring the stabilization of a time-varying\n",
      "desired joint trajectory, as well as joint limit avoidance, in the case of\n",
      "fully-actuated manipulators. The key idea is to perform a parametrization of\n",
      "the feasible joint space in terms of exogenous states. It follows that the\n",
      "control of these states allows for joint limit avoidance. One of the main\n",
      "outcomes of this paper is that position terms in control laws are replaced by\n",
      "parametrized terms, where joint limits must be avoided. Stability and\n",
      "convergence of time-varying reference trajectories obtained with the proposed\n",
      "method are demonstrated to be in the sense of Lyapunov. The introduced control\n",
      "laws are verified by carrying out experiments on two degrees-of-freedom of the\n",
      "humanoid robot iCub.\n",
      "0 0.982688615601\n"
     ]
    }
   ],
   "source": [
    "score, best_i = return_max_resembling_paper()\n",
    "print(\">>> Best score is:\", score)\n",
    "print(\">>> Abstract of the closest paper:\")\n",
    "print(s[best_i])\n",
    "print(\">>> Abstract of initial paper:\")\n",
    "print(new_doc[0])\n",
    "for i, item in enumerate(topic_distribution([s[best_i]])):\n",
    "    if item > 0:\n",
    "        print(i, item) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the paper looks relevant although the score is not very high (~0.69). In fact, scores very close to 1 would be suspicious (plagiarism?).\n",
    "\n",
    "This is **very first step** (I started the project yesterday, 28-Oct-2017), so a number of improvements should be made, such as\n",
    "- more professional wording (including 2-grams), \n",
    "- more detailed topic modeling, \n",
    "- faster implementation,\n",
    "- website and (or) app\n",
    "\n",
    "are clearly missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
